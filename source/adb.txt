Deep Learning for IoT with Edge Computing
Abstract
This article discusses the motivations behind using the technique of deep learning to process information from IoT devices, and how doing this computation near the edge of the networking will prove to be better than the traditional method of cloud computing. It also presents a relatively new method to optimize the deep learning tasks performed and also proves that it surpasses different other methods, by doing a performance analysis.
Index

Introduction
The introduction can be outlined in the following different headings. First, we will discuss the applications of deep learning in IoT technologies. In the following section we will talk about edge computing. Finally, we will introduce the idea and advantages of implementing deep learning technologies in edge computing.
Deep learning in IoT
Deep learning has only recently become popular in the research field and is starting to be used in many such areas. While the method of machine learning has been known for quite some time and is also proven to be very useful, it has been hard to apply the techniques in gathering data from IoT devices collected from the real world. Deep learning has helped to counter most of these problems. As the IoT devices and applications produce huge amounts of data, deep learning is perfectly suited for this problem. For example, an IoT based deep learning system is used to obtain extremely accurate weather forecasting data(ref1). An important characteristic of the deep learning model is the presence of multiple network layers, which reduce the size of data passing through each layer, as the system keeps learning new features about the data throughout. We will see how this is helpful in the later subsections.
Edge computing
Edge computing is an innovative paradigm, where processing takes place near the edge layer of the network. As the data collected from the world through various devices and applications is becoming increasingly larger by the day, it is also becoming difficult to transfer this data to the servers in the cloud, for computation. Limited network availability and restricted bandwidth make it harder for transmission of data from IoT devices to the centralized cloud. Hence, edge computing is considered to be a viable replacement to this conventional method. It works especially well when the size of the preprocessed data is significantly smaller compared to the size of the initial data extracted.
Combination of Deep learning and edge computing
From the above two sections we can infer that applying Deep learning techniques in tandem with edge computing will offer better results than other alternatives. Deep learning system is suitable for edge computing networks as the intermediate data processed in the edge nodes can be passed through the learning layers into the cloud.  The deep learning process would also ensure security and privacy as the intermediate data will have different semantics in comparison with the input data.  Finally, to maximize the performance of the model we implement, we will increase efficiency of scheduling by countering the problems of limited bandwidth of network and restricted functions of the edge servers. In the further sections of the article, we will explain how this is done.
Motivation behind relating technologies and means to implement
As we had discussed in the introduction, one important benefit of using deep learning algorithms is that it can process a large amount of data efficiently, which is particularly valuable as the IoT devices are a source for such huge figures. In comparison with machine learning, deep learning takes a lot less time in training and can also be taught to precisely learn new features. As we know, the deep learning network is divided into multiple layers. The layers near the source are called lower layers and the rest are higher layers. Each of these layers acquires features from the previous layers along with learning new ones.  But a few problems are posed when applying the deep learning methods to the IoT devices, as the typical firmware built into the systems is insufficient in supporting the deep learning tasks. Experimental results from N. D. Lane et al. prove that this can be handled by using new acceleration engines, like DeepEar and DeepX to support the intended applications in the IoT systems on high spec chips (SoCs)(ref2).
Another significant hurdle in utilizing deep learning applications is that most still require cloud support. To address this issue, M. A. Alsheikh et al. combined deep learning algorithms with Apache Spark (Analytics engine) to analyse data from IoT devices.  This was further divided into separate layers such that the Apache Spark computation was carried out in the cloud servers, whereas the extraction of data was done in the mobile IoT device. This is very similar to what edge computing does and is evidence for the fact that it is possible to execute processing tasks in the mobile phase itself instead of the cloud. The IoT network can primarily be divided into two layers â€“ Edge layer and cloud layer. The edge layer is comprised of the mobile IoT devices, the IoT gateway nodes and the network access points, while the cloud layer consists of all other network connections and the cloud servers. Edge computing allows processing to be performed in the edge layer itself instead of the cloud layer. This pre-processing in the edge nodes can relieve the burden of transferring the large amounts of data to the cloud servers. Edge computing is convenient in processes where the size of intermediate data is smaller than the source data, making it perfect for its use with deep learning techniques.
The first application of deep learning in the edge computing environment was when C. Liu proposed the development of a novel deep learning based visual food recognition system employing edge computing services to achieve an accurate dietary assessment with real-world data. This method used mobile phones as edge nodes, but since we have to consider general IoT devices with lower specifications, we need to modify our algorithm accordingly. Thus, we set up the edge servers in the IoT gateways for processing the gathered data. Then the deep learning model is trained in the cloud server. Next, the lower layers of the learning network are deployed into the edge servers and the higher layers into the cloud servers. Finally, as we already know, the intermediate data is transferred from the edge servers to the higher layers into the cloud.

The Scheduling problem and solutions

  The main obstruction we encounter in our model is that the edge servers are only capable of processing a limited number of tasks with a limited number of deployed learning network layers. Since the size of data processed by lower layers are usually larger than the higher layers, the above problem does not allow most of the computation to happen in the edge servers to reduce load on network traffic. Along with this, deep learning networks and applications have different sizes of pre-processed data and also some excess computational time associated with them, which has to be handled by efficient scheduling methods.
This issue is solved by proposing a scheduling problem and then solving it with a well-known algorithm(reftopaper). This problem attempts to achieve maximum tasks performed in the edge computing environment by adding deep learning network layers in the edge servers of IoT devices, such that the latency/delay in transferring data through the network is minimised. This is denoted in the equations given below.

To accomplish this, two algorithms are created, offline and online. The offline algorithm, in essence, finds the precise value of the number of layers (k) for processing tasks in all edge servers. If the network bandwidth is adequate, it will deploy the task in all edge servers, otherwise if the edge server is incapable of operating the task, the algorithm will not deploy the task. The time complexity of this algorithm, with approximations, is found to be O(|T|), where T is the set of all deep learning tasks. Since, this is an approximation algorithm, the efficiency is expressed in terms of a performance ratio, which in this case is found to be 2/V, where V is the maximum threshold value for the network bandwidth, to prevent congestion.
Along with the offline one, an online algorithm is also designed which determines when to deploy the deep learning task into the edge servers based on the previous tasks encountered.
(write the stuff important here)

Performance analysis of solution
Now, we will make a performance evaluation and examine the result of our presented solution. First, we will execute deep learning tasks in Convolutional Neural Networks (CNNs) and note down the number of operations and the intermediate data size processed in every CNN layer. Fig.2 shows the decrease in the data size ratio as more and more CNN layers are deployed. On the other hand, the computational overhead increases as the layers are added.
For the analysis of the offline scheduling algorithm, we compare it to the fixed layer deploying system, adding a predetermined amount of learning layers into the edge servers. Observing from the graph in Fig.3 the scheduling algorithm gives a better result than the fixed mode, which deploys 1, 2, 3, 4 and 5 layers in each plot. The highest among the fixed mode plots is the one for two layers. This may be due to the fact that in most of the deep learning models, two layers strike an accurate balance between reduced intermediate data size and increase in computational overhead.
Next, we move on to evaluating the performance of the online scheduling algorithm along with two other popular online algorithms, FIFO (First In First Out) and LBF (Lowest Bandwidth First). The FIFO algorithm works by deploying all the tasks up to the full service capability of the edge servers and if it exceeds the limit, it pops out the tasks deployed at first to accommodate the addition of tasks. The LBF algorithm, instead pops out the task taking up the maximum bandwidth when not capable of deploying other tasks. But as seen from the graph in Fig.4 the online scheduling algorithm does a better job of deploying tasks than both FIFO and LBF for significantly longer periods of time.
Conclusion and Future Scope
As mentioned throughout, the use of deep learning algorithms for the IoT proves to be very efficient in utilizing the network resources judiciously along with ensuring privacy of data. Performing this in the edge computing environment is further beneficial as it avoids network congestion and relaxes the load on the centralized cloud servers.
We also take the limited operations capability of the edge nodes into consideration and propose scheduling algorithms to ensure maximum performance. The results of our evaluation are evident enough to show that our algorithms are effective and can prove to be very valuable in many different fields in the future.
